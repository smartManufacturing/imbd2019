{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imbd2019.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwlAnC8mKKWt"
      },
      "source": [
        "修正Tersonflow與Keras版本"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oryu7yWhF3in"
      },
      "source": [
        "!pip uninstall keras\n",
        "!pip install tensorflow==2.2.0\n",
        "!pip install Keras==2.3.1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baWEgaKBG4h3"
      },
      "source": [
        "匯入雲端硬碟"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VDsM0gWG8uQ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls /content/drive/My\\ Drive/Colab\\ Notebooks/imbd2019\n",
        "import sys\n",
        "sys.path.append('content/drive/My Drive/Colab Notebooks/imbd2019/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CxJ6ifbG-k7"
      },
      "source": [
        "載入配置"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLABH9QjXvIi"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3Q3J6j2HEHY"
      },
      "source": [
        "class _config:\n",
        "    # train 資料夾位置\n",
        "    #Train_Path = './train/'\n",
        "    Train_Path = '/content/drive/My Drive/Colab Notebooks/imbd2019/train/' \n",
        "    # test 資料夾位置\n",
        "    #Test_Path = './test/'\n",
        "    Test_Path = '/content/drive/My Drive/Colab Notebooks/imbd2019/test/'\n",
        "    # img size\n",
        "    imgrow = 164\n",
        "    imgcol = 4\n",
        "    # PTC 類別定義\n",
        "    PTC_class = {'G11': 0, 'G15': 1, 'G17': 2, 'G19': 3,\n",
        "                 'G32': 4, 'G34': 5, 'G48': 6, 'G49': 7}\n",
        "    test_y = {'1.txt': 'G11', '2.txt': 'G11',\n",
        "              '3.txt': 'G15', '4.txt': 'G15', '5.txt': 'G15', '6.txt': 'G15', '7.txt': 'G15', '8.txt': 'G15',\n",
        "              '9.txt': 'G17', '10.txt': 'G17',\n",
        "              '11.txt': 'G19', '12.txt': 'G19',\n",
        "              '13.txt': 'G32', '14.txt': 'G32', '15.txt': 'G32', '16.txt': 'G32', '17.txt': 'G32', '18.txt': 'G32',\n",
        "              '19.txt': 'G34', '20.txt': 'G34', '21.txt': 'G34', '22.txt': 'G34', '23.txt': 'G34', '24.txt': 'G34',\n",
        "              '25.txt': 'G48', '26.txt': 'G48', '27.txt': 'G48', '28.txt': 'G48', '29.txt': 'G48', '30.txt': 'G48',\n",
        "              '31.txt': 'G49', '32.txt': 'G49', '33.txt': 'G49', '34.txt': 'G49', '35.txt': 'G49', '36.txt': 'G49',\n",
        "              }\n",
        "    Batch_size = 32\n",
        "    epoch = 70\n",
        "    class_num = 8\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00Dfo0MCHN_R"
      },
      "source": [
        "載入檔案"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTIAW29pHfd8"
      },
      "source": [
        "import sys\n",
        "from os import walk\n",
        "import numpy as np\n",
        "import cv2\n",
        "#from config import _config\n",
        "\n",
        "class _file:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.Set = _config()\n",
        "        pass\n",
        "\n",
        "    def ReadData_TxT(self, Path):\n",
        "        AllPTCNp = np.zeros((1, self.Set.imgcol))\n",
        "        AllDataList = list()\n",
        "        labellsit = list()\n",
        "        PTClabelList = list()\n",
        "        namelist = list()\n",
        "        # 讀取資料夾內檔案\n",
        "        for root, dirs, file_all in walk(Path):\n",
        "\n",
        "            for _file in dirs:\n",
        "                # 讀取各類別資料夾(G11、G15、G17、G19、G32、G34、G48、G49)\n",
        "                for root, dirs, file_all in walk(Path+_file):\n",
        "                    # 讀取各類別資料夾內的TXT\n",
        "                    for name in file_all:\n",
        "                        # print(name)\n",
        "                        namelist.append(name)\n",
        "                        Datalist = list()\n",
        "                        SingleTxTNp = np.zeros((1, 4))\n",
        "                        PTCList = list()\n",
        "                        StartSaveData = False\n",
        "\n",
        "                        # 讀取TXT\n",
        "                        f = open(Path+_file+'/'+name, 'r', encoding='big5')\n",
        "                        data = f.readlines()\n",
        "                        for i in data:\n",
        "                            Datalist.append(i)\n",
        "                        f.close()\n",
        "\n",
        "                        # 讀取TXT內容後，淨化資料內容\n",
        "                        for i in Datalist:\n",
        "                            # 判斷此TXT類別\n",
        "                            if i.find('G') > -1:\n",
        "                                label = i[i.find('G'):i.find('-')]\n",
        "\n",
        "                                labellsit.append(label)\n",
        "                            # 經過Deg.F 確認開始讀取 PTC\n",
        "                            if StartSaveData:\n",
        "                                PTCList.append(i)\n",
        "\n",
        "                                # 若讀到換行符號，row方向往下堆疊\n",
        "                                if i.find('\\n') >= 0:\n",
        "                                    PTCList = PTCList[0].split('\\t')\n",
        "                                    showNp = np.array(PTCList)\n",
        "\n",
        "                                    PTCList = []\n",
        "                                    if showNp.shape[0] > 1:  # txt開頭有換行符號則須避開\n",
        "                                        SingleTxTNp = np.vstack(\n",
        "                                            (SingleTxTNp, showNp[0:self.Set.imgcol]))  # 資料往下堆疊\n",
        "                            if i.find('Deg.F') > -1:  # 經過Deg.F 確認開始讀取 PTC\n",
        "                                StartSaveData = True\n",
        "\n",
        "                        # 去除SingleTxTNp row=0資料，只留實際PTC資料\n",
        "                        SingleTxTNp = SingleTxTNp[1:self.Set.imgrow, :]\n",
        "                        SingleTxTNp_float = SingleTxTNp.astype(\n",
        "                            np.float)  # convert string array to float\n",
        "\n",
        "                        AllPTCNp = np.vstack(\n",
        "                            (AllPTCNp, SingleTxTNp_float))  # 讀完整個txt內容後存入結果\n",
        "                # 去除AllPTCNp row=0資料，只留實際PTC資料\n",
        "                ResultNp = AllPTCNp[1:AllPTCNp.shape[0], :].copy()\n",
        "\n",
        "        # 儲存Label資料\n",
        "        for i in labellsit:\n",
        "            i = i.strip(' ')\n",
        "            PTClabelList.append(self.Set.PTC_class[i])\n",
        "        PTClabelNp = np.array([PTClabelList])\n",
        "        PTClabelNp = PTClabelNp.T\n",
        "        print(ResultNp.shape)\n",
        "\n",
        "        return ResultNp, PTClabelNp, namelist\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPLkLMh5Hhpt"
      },
      "source": [
        "資料前處理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJJzNc2iHqdj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class _preprocess:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def splitdata(self, dataNp, ratio, imgrow):\n",
        "        # input資料row維度\n",
        "        imgcounter = int(dataNp.shape[0]/imgrow)\n",
        "        # train維度=all*ratio\n",
        "        trainCounter = int(imgcounter*ratio)\n",
        "        trainNp = dataNp[0:trainCounter*imgrow, :]\n",
        "\n",
        "        # test=all-train\n",
        "        testNp = dataNp[trainCounter*imgrow:imgcounter*imgrow, :]\n",
        "\n",
        "        return trainNp, testNp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0MmsErwHtTf"
      },
      "source": [
        "訓練模型配置"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBBdoDHDHx9_"
      },
      "source": [
        "from __future__ import print_function\n",
        "import sys\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense, Activation, Bidirectional\n",
        "#from tensorflow.keras.optimizers import Adam\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import LSTM\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Conv1D, MaxPooling1D, GlobalAveragePooling1D, MaxPool2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "#from keras.layers.normalization import BatchNormalization\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam, Adadelta\n",
        "#from preprocess import _preprocess\n",
        "#from config import _config\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import pickle\n",
        "import gzip\n",
        "import urllib.request\n",
        "import json\n",
        "\n",
        "def plothistory(history):\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "# 繪製訓練 & 驗證的損失值\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class CNN_Model:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        pass\n",
        "\n",
        "    def Classification(self, x_train_all_Np, y_train_all_Np, x_testNp, y_testNp):\n",
        "        Set = _config()\n",
        "        process = _preprocess()\n",
        "\n",
        "        # train data分80%,test data分20%\n",
        "        train_x_np, val_x_np = process.splitdata(x_train_all_Np, 0.8, 1)\n",
        "        train_y_np, val_y_np = process.splitdata(y_train_all_Np, 0.8, 1)\n",
        "        print(train_x_np.shape, val_x_np.shape,\n",
        "              train_y_np.shape, val_y_np.shape)\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = Set.imgrow-1, Set.imgcol\n",
        "        train_x_np = train_x_np.reshape(\n",
        "            int(train_x_np.shape[0]/(img_rows)), img_rows, img_cols, 1)\n",
        "        val_x_np = val_x_np.reshape(\n",
        "            int(val_x_np.shape[0]/(img_rows)), img_rows, img_cols, 1)\n",
        "        x_train_all_Np = x_train_all_Np.reshape(\n",
        "            int(x_train_all_Np.shape[0]/(img_rows)), img_rows, img_cols, 1)\n",
        "        # print(x_train_all_Np.shape,x_testNp.shape)\n",
        "\n",
        "        _input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        # Normalize\n",
        "        x_mean = np.mean(x_train_all_Np, axis=0)\n",
        "        x_std = np.std(x_train_all_Np, axis=0)\n",
        "\n",
        "        train_x_np = (train_x_np-x_mean)/x_std\n",
        "        val_x_np = (val_x_np-x_mean)/x_std\n",
        "\n",
        "        print('x_train shape:', x_train_all_Np.shape)\n",
        "        print(train_x_np.shape, 'train samples')\n",
        "        print(val_x_np.shape, 'val samples')\n",
        "        print(train_y_np.shape, 'trainy samples')\n",
        "        print(val_y_np.shape, 'valy samples')\n",
        "        print(train_y_np)\n",
        "        train_y_np_Onehot = keras.utils.to_categorical(\n",
        "            train_y_np, Set.class_num)\n",
        "        val_y_np_Onehot = keras.utils.to_categorical(val_y_np, Set.class_num)\n",
        "        print('train_y_np_Onehot', train_y_np_Onehot.shape,\n",
        "              'val_y_np_Onehot', val_y_np_Onehot.shape)\n",
        "\n",
        "        print('_input_shape', _input_shape)\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(64, kernel_size=(1, 1),\n",
        "                         activation='relu',\n",
        "                         #  padding='SAME',\n",
        "                         input_shape=_input_shape))\n",
        "        # model.add(Conv2D(64, kernel_size=(1, 1),activation='relu'))#\n",
        "        # model.add(Conv2D(64, kernel_size=(1, 1),activation='relu'))#\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Conv2D(64, kernel_size=(1, 1), activation='relu'))\n",
        "        # # model.add(Conv2D(64, kernel_size=(1, 1),activation='relu'))#\n",
        "        # # model.add(Conv2D(64, kernel_size=(2, 2),activation='relu'))#\n",
        "\n",
        "        # model.add(Conv2D(64, kernel_size=(1, 1),activation='relu'))#\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        # model.add(Conv2D(64, kernel_size=(1, 1),activation='relu'))#\n",
        "\n",
        "        # model.add(Conv2D(64, kernel_size=(1, 1),activation='relu'))#\n",
        "\n",
        "        # model.add(MaxPooling2D(pool_size=(2, 2)))#\n",
        "\n",
        "        # model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "        # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        # model.add(Dropout(0.25))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "        # model.add(Dense(512, activation='relu'))\n",
        "        model.add(Dense(512, activation='relu'))\n",
        "        # model.add(Dense(1024, activation='relu'))\n",
        "        model.add(Dense(1024, activation='relu'))\n",
        "        model.add(Dense(2048, activation='relu'))\n",
        "        # model.add(Dense(2048, activation='relu'))\n",
        "        # model.add(Dense(2048, activation='relu'))\n",
        "        # model.add(Dense(1024, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(Set.class_num, activation='softmax'))\n",
        "\n",
        "        model.summary()\n",
        "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        # model.fit(x_train_all_Np, y_train_all_Np_Onehot, validation_split=0.2, batch_size=32, epochs=50)\n",
        "        print('t', val_x_np.shape, val_y_np_Onehot.shape)\n",
        "\n",
        "        history = model.fit(train_x_np,\n",
        "                            train_y_np_Onehot,\n",
        "                            # validation_split=0.5,\n",
        "                            batch_size=Set.Batch_size,\n",
        "                            epochs=Set.epoch,\n",
        "                            # verbose=1)\n",
        "                            validation_data=(val_x_np, val_y_np_Onehot))\n",
        "        # score = model.evaluate(x_testNp, y_testNp_Onehot)\n",
        "        score = model.evaluate(val_x_np, val_y_np_Onehot, verbose=0)\n",
        "\n",
        "        x_testNp = x_testNp.reshape(\n",
        "            int(x_testNp.shape[0]/(img_rows)), img_rows, img_cols, 1)\n",
        "        x_testNp = (x_testNp-x_mean)/x_std\n",
        "\n",
        "        predction = model.predict_classes(x_testNp)\n",
        "        # print(predction)\n",
        "        print('Test loss:', score[0])\n",
        "        print('Test accuracy:', score[1])\n",
        "        # plothistory(history)\n",
        "\n",
        "        return predction\n",
        "\n",
        "\n",
        "class LSTM_Model:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def Classification(self, x_train_all_Np, y_train_all_Np, x_testNp, y_testNp):\n",
        "        Set = _config()\n",
        "        process = _preprocess()\n",
        "        # learning_rate = 0.001\n",
        "        training_iters = Set.epoch\n",
        "        batch_size = Set.Batch_size\n",
        "        display_step = 10\n",
        "\n",
        "        n_input = Set.imgcol\n",
        "        n_step = Set.imgrow-1\n",
        "        n_hidden = 256\n",
        "        n_classes = Set.class_num\n",
        "\n",
        "        train_x_np, val_x_np = process.splitdata(x_train_all_Np, 0.8, 1)\n",
        "        train_y_np, val_y_np = process.splitdata(y_train_all_Np, 0.8, 1)\n",
        "\n",
        "        x_train_all_Np = x_train_all_Np.reshape(-1, n_step, n_input)\n",
        "        train_x_np = train_x_np.reshape(-1, n_step, n_input)\n",
        "        val_x_np = val_x_np.reshape(-1, n_step, n_input)\n",
        "\n",
        "        # train_x_np = train_x_np.astype('float32')\n",
        "        # val_x_np = val_x_np.astype('float32')\n",
        "\n",
        "        x_mean = np.mean(x_train_all_Np, axis=0)\n",
        "        x_std = np.std(x_train_all_Np, axis=0)\n",
        "\n",
        "        train_x_np = (train_x_np-x_mean)/x_std\n",
        "        val_x_np = (val_x_np-x_mean)/x_std\n",
        "\n",
        "        train_y_np_Onehot = keras.utils.to_categorical(train_y_np, n_classes)\n",
        "        val_y_np_Onehot = keras.utils.to_categorical(val_y_np, n_classes)\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(n_hidden, batch_input_shape=(\n",
        "            None, n_step, n_input), unroll=True))\n",
        "        # model.add(LSTM(n_hidden,unroll=True))\n",
        "        # model.add(LSTM(n_hidden))\n",
        "\n",
        "        # model.add(LSTM(n_hidden,batch_input_shape=(None, n_step, n_input),unroll=True))\n",
        "\n",
        "        # model.add(Bidirectional(LSTM(units=2,return_sequences=False)))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(n_classes))\n",
        "        model.add(Activation('softmax'))\n",
        "\n",
        "        adam = Adam(lr=0.001)\n",
        "        model.summary()\n",
        "        model.compile(optimizer=adam,\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        model.fit(train_x_np, train_y_np_Onehot,\n",
        "                  batch_size=batch_size,\n",
        "                  epochs=training_iters,\n",
        "                  verbose=1,\n",
        "                  validation_data=(val_x_np, val_y_np_Onehot))\n",
        "\n",
        "        scores = model.evaluate(val_x_np, val_y_np_Onehot, verbose=2)\n",
        "        print('LSTM test score:', scores[0])\n",
        "        print('LSTM test accuracy:', scores[1])\n",
        "\n",
        "        x_testNp = x_testNp.reshape(-1, n_step, n_input)\n",
        "        x_testNp = (x_testNp-x_mean)/x_std\n",
        "        predction = model.predict_classes(x_testNp)\n",
        "        print(predction)\n",
        "        return predction\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emh3ngEcH4ou"
      },
      "source": [
        "主程式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRYs1oheH-wm"
      },
      "source": [
        "#from file import _file\n",
        "#from config import _config\n",
        "#from Mymodel import CNN_Model, LSTM_Model\n",
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "from tkinter import font\n",
        "import os\n",
        "\n",
        "def Project_Classification():\n",
        "    tXt = _file()\n",
        "    Set = _config()\n",
        "    # 讀取train data 及 test data\n",
        "    train_x, train_y, trainfilename = tXt.ReadData_TxT(Set.Train_Path)\n",
        "    test_x, test_y, testfilename = tXt.ReadData_TxT(Set.Test_Path)\n",
        "    print('=======', train_x.shape, train_y.shape)\n",
        "    # print(testfilename)\n",
        "    Real_y = list()\n",
        "    for i in testfilename:\n",
        "        _class = Set.test_y[i]\n",
        "        Real_y.append(Set.PTC_class[_class])\n",
        "    CNN = CNN_Model()\n",
        "    Predict_Y = CNN.Classification(train_x, train_y, test_x, test_y)\n",
        "    # goolgenet=goolgenet_Model()\n",
        "    # goolgenet.Classification(train_x,train_y,test_x,test_y)\n",
        "    # LSTM=LSTM_Model()\n",
        "    # Predict_Y=LSTM.Classification(train_x,train_y,test_x,test_y)\n",
        "\n",
        "    Real_yNp = np.array(Real_y)\n",
        "    print('predict:', Predict_Y)\n",
        "    print('Real:   ', Real_yNp)\n",
        "    Result = 0\n",
        "    for i in range(Real_yNp.shape[0]):\n",
        "        if(Predict_Y[i] == Real_yNp[i]):\n",
        "            Result = Result+1\n",
        "    Error = Result/Real_yNp.shape[0]\n",
        "    print('Accuracy(%):', Error*100)\n",
        "\n",
        "\n",
        "def main():\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # 使用 GPU 0\n",
        "    Project_Classification()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}